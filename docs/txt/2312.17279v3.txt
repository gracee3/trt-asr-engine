===== FILE: ./2312.17279v3.pdf | PAGE: 1/8 =====

STATEFUL CONFORMER WITH CACHE-BASED INFERENCE FOR STREAMING
AUTOMATIC SPEECH RECOGNITION
Vahid Noroozi1
, Somshubra Majumdar1
, Ankur Kumar2
, Jagadeesh Balam1
, Boris Ginsburg1
1
NVIDIA
2
UCLA, Department of Computer Science
ABSTRACT
In this paper, we propose an efficient and accurate stream-
ing speech recognition model based on the FastConformer
architecture. We adapted the FastConformer architecture
for streaming applications through: (1) constraining both
the look-ahead and past contexts in the encoder, and (2)
introducing an activation caching mechanism to enable the
non-autoregressive encoder to operate autoregressively dur-
ing inference. The proposed model is thoughtfully designed
in a way to eliminate the accuracy disparity between the
train and inference time which is common for many stream-
ing models. Furthermore, our proposed encoder works with
various decoder configurations including Connectionist Tem-
poral Classification (CTC) and RNN-Transducer (RNNT)
decoders. Additionally, we introduced a hybrid CTC/RNNT
architecture which utilizes a shared encoder with both a CTC
and RNNT decoder to boost the accuracy and save computa-
tion.
We evaluate the proposed model on LibriSpeech dataset
and a multi-domain large scale dataset and demonstrate that it
can achieve better accuracy with lower latency and inference
time compared to a conventional buffered streaming model
baseline. We also showed that training a model with multi-
ple latencies can achieve better accuracy than single latency
models while it enables us to support multiple latencies with a
single model. Our experiments also showed the hybrid archi-
tecture would not only speedup the convergence of the CTC
decoder but also improves the accuracy of streaming models
compared to single decoder models.
Index Terms--- Streaming ASR, FastConformer, Con-
former, CTC, RNNT
1. INTRODUCTION
Many of the traditional end-to-end streaming automatic
speech recognition (ASR) models use auto-regressive RNN-
based architectures [1] as we don't have access to all the
future speech in streaming mode. Offline ASR models can
potentially use the global context while streaming ASR mod-
els need to use a limited future context which degrades their
He worked on this paper while doing an internship at NVIDIA.
accuracy compared to offline models. In some streaming ap-
proaches, offline models are being used for streaming which
would be another source of accuracy degradation as there is
inconsistency between offline training and streaming infer-
ence. The accuracy gap between streaming and offline models
can be reduced by using large overlapping buffers where left
and right contexts are added to each chunk of audio, how-
ever, this requires significant redundant computations for
overlapping segments.
In this paper, we propose an efficient and accurate stream-
ing model based on the FastConformer [2] architecture which
is a more efficient variant of Conformer[3]. Our proposed
approach would work with both the Conformer and FastCon-
former architectures but we performed our experiments with
just FastConformer as it more than 2X faster than Conformer.
We also introduced a hybrid CTC/RNNT architecture with
two decoders of CTC [4] and RNNT [5] with a shared en-
coder. It would not only saves computation as a single model
is trained instead of two separate models but also improves
the accuracy and convergence speed of the CTC decoder.
We propose a caching mechanism by converting the Fast-
Conformer's non-autoregressive encoder into an autoregres-
sive recurrent model during inference using a cache for acti-
vations computed from previous timesteps. A cache stores the
intermediate activations which are reused in future steps. The
caching removes the requirement of any buffer or overlap-
ping chunks which results in avoiding any unnecessary dupli-
cate computations. It drastically reduces the computation cost
when compared to traditional buffer-based methods. Note
that the model is still trained efficiently in non-autoregressive
mode, similar to offline models.
The model has also limited right and left contexts dur-
ing training to maintain consistent conditions during training
and streaming inference. This consistency helps to reduce
the accuracy gap between offline inference and streaming
inference significantly. Additionally, as the changes are
limited to the encoder architecture, the proposed approach
works for both FastConformer-CTC and FastConformer-
Transducer (FastConformer-T) models. We evaluate the
proposed streaming model on the LibriSpeech dataset and a
large multi-domain dataset and show that it outperforms the
buffered streaming approaches in terms of accuracy, latency,
arXiv:2312.17279v3
[cs.CL]
2
May
2024

===== FILE: ./2312.17279v3.pdf | PAGE: 2/8 =====

and inference time. We also study the effect of the right
context on the trade-off between latency and accuracy. In
another experiment, we would evaluate a model trained with
multiple latencies which can support multiple latencies in a
single model. In our experiments, we show that it can achieve
better accuracy than models trained with single latency. Ad-
ditionally we show that our hybrid architecture can achieve
better accuracy compared to single decoder models with less
compute. All the code and models used in the paper including
the training and inference scripts are open-sourced in NeMo
[6] toolkit1
.
2. RELATED WORKS
There are a number of approaches that use limited future con-
text in streaming models. The time-restricted methods in [7,
8] use masking in each layer to allow a limited look-ahead for
each output token. However, these methods are not compu-
tationally efficient since the computations for look-ahead to-
kens are discarded and they need to be recomputed again for
future steps. Another approach is based on splitting the input
audio into several chunks. Each output token corresponding
to a chunk has access to all input tokens in the current chunk
as well as a limited number of previous chunks. This ap-
proach is more efficient and accurate than the time-restricted
method [9].
Some memory-based approaches [10, 11, 12] use con-
textual memory to summarize older chunks into a vector to
be used in the subsequent chunks. For example, a stream-
ing Transformer model in [10] with an attention-based en-
coder/decoder (DEA) architecture uses a context embedding
to maintain some memory state between consecutive chunks.
Generally, these techniques are computationally efficient for
inference, but they usually break the parallel nature of train-
ing, resulting in less robust and efficient training.
There exist a number of previous works that adopted Con-
former for streaming ASR [13, 14, 15]. In [13, 14], the au-
thors have developed a unified model which can work in both
streaming and non-streaming modes. Yao et al. [15] pro-
posed a streaming Conformer that uses a Transformer de-
coder. Their model supports dynamic look-ahead by training
the model with different look-ahead sizes.
3. CACHE-AWARE STREAMING
FASTCONFORMER
In our proposed cache-aware streaming FastConformer, the
left and right contexts of each audio step are controlled and
limited. It enables us to have consistent behavior during both
training and inference. The proposed model is trained in an
efficient non-autoregressive manner, but inference is done in
an autoregressive, recurrent way.
1https://github.com/NVIDIA/NeMo
The original FastConformer encoder consists of self-
attention, convolutions, and linear layers. The linear layers
and 1D convolutions with kernel size of one do not need any
context because their outputs in each step are just dependent
on that step. However, self-attention and convolutions with
kernel size larger than one need context, and we need to limit
the context for these specific layers to control the context size
of the whole model.
3.1. Model training
We modify the FastConformer model as follows to adapt it
for the streaming scenario. We avoid using normalization in
the mel-spectrogram feature extraction step as the normaliza-
tion procedure need to use of some statistics which depend
on the entire input audio. We make all the convolution lay-
ers including those in the downsampling layers fully causal.
For this purpose, we use padding of size k - 1 to the left
of the input sequence where k is the convolution kernel size,
and zero padding for the right side. From now onwards, we
will drop the ``downsampled'' prefix and simply refer to the
``downsampled input'' as ``input''. We replace all the batch
normalization [16] layers with layer normalization [17] as the
former computes mean and variance statistics from the entire
input sequence whereas the latter normalizes each step of the
input sequence independently.
There are three approaches to limit the size of the context
for self-attention layers:
Zero look-ahead: Zero look-ahead means each step in
the sequence has access only to previous tokens (either all
past tokens or only a subset of them). This is crucial for
low-latency applications. Therefore, all modules need to be
causal including the self-attention layers. We use masking
to ignore the contribution of all future tokens in the attention
score computation. It results in a small latency and inference
time but lower prediction accuracy.
Regular look-ahead: It has been shown that having ac-
cess to some future time steps, i.e. limited look-ahead, can
significantly improve the accuracy of an ASR model [9]. The
simplest approach is to allow a small look-ahead in each self-
attention layer [8, 7]. Since layers are stacked, the effective
look-ahead gets multiplied by the number of self-attention
layers as we move deeper in the network as shown in Fig-
ure 1(a). For example, in a model with N self-attention lay-
ers, where each one has a look-ahead of M, the effective
look-ahead of each output token over the input sequence is
M \times\\{\\} N. The effective look-ahead directly impacts the final
latency since the model needs to wait for M \times\\{\\} N time steps
before it can make any prediction. The past context in this ap-
proach can be any number of tokens. But allowing larger past
context will increase the computation time for each streaming
step.
Chunk-aware look-ahead: There are two disadvantages
to the regular look-ahead. First, the effective overall look-

===== FILE: ./2312.17279v3.pdf | PAGE: 3/8 =====

fk-9 fk-8 fk-7 fk-6 fk-5 fk-4 fk-3 fk-2 fk-1 fk fk+1 fk+2 fk+3 fk+4 fk+5
hk-9 hk-8 hk-7 hk-6 hk-5 hk-4 hk-3 hk-2 hk-1 hk hk+1 hk+2 hk+3 hk+4 hk+5
fk-9 fk-8 fk-7 fk-6 fk-5 fk-4 fk-3 fk-2 fk-1 fk fk+1 fk+2 fk+3 fk+4 fk+5
(a) Regular
hk+2 hk+3 hk+4 hk+5
hk-9 hk-8 hk-7 hk-6 hk-5 hk-4 hk-3 hk-2 hk-1 hk hk+1
Chunk i-3 Chunk i-2 Chunk i-1 Chunk i Chunk i+1
(b) Chunked
Fig. 1. Diagram of how context gets extended with multi-
layer self-attention layers in regular look-ahead vs chunk-
aware. Dependency on future frames increases for regular
look-ahead in self-attention layers as we go deep in the net-
work whereas it remains the same for chunk-aware approach.
ahead depends on the number of layers having non-zero look-
ahead. Thus, the latency can be significant if we use look-
ahead in each layer. Even for a reasonably large latency bud-
get, we can only use a small look-ahead size in each layer
(which we denote M). For example, for a model with 17 lay-
ers, a frame rate of 10ms, and subsampling factor of 4, choos-
ing look-ahead size M = 2 results in a latency of 10 \times\\{\\} 4 \times\\{\\}
17 \times\\{\\} 2 = 1360 milliseconds. Therefore, M cannot be much
larger for practical applications [18].
The other disadvantage of regular look-ahead is the un-
necessary re-computation of some tokens during the stream-
ing inference. For example, to compute fk in figure 1(a), the
self-attention operation is applied on blue tokens with a query
size of M + 1, 1 for current timestep k and M for future
tokens (more details in section 3.3). This generates the gray-
shaded token fk+1 along with the desired output fk shown in
yellow. But we drop fk+1 generated in this step as it is not
correct due to its dependency on hk+3, which is not available
yet. Therefore, we need to recompute fk+1 along with other
such tokens across different layers.
Chunk-aware look-ahead [9, 19, 20] addresses both the
above issues. It splits the input audio into chunks of size C.
Tokens in a chunk have access to all other tokens in the same
chunk as well as those belonging to a limited number of previ-
ous chunks. In contrast with the effective look-ahead growing
with depth in regular look-ahead, there is no such dependency
in chunk-aware look-ahead. Due to chunking, the output pre-
dictions of the encoder for all the tokens in each chunk will
be valid and there is no need to recompute any activation for
FastConformer
Encoder
CTC
Decoder
RNNT
Decoder
RNNT
Loss
CTC Loss
Loss
Mixer
Total
Loss
Fig. 2. Architecture of the hybrid CTC/RNNT model.
the future tokens. This results in zero duplication in the com-
pute and makes the inference efficient. While the look-ahead
of each token is the same for regular look-ahead by construc-
tion, it varies in the range

0, C-1

for the chunk-aware case.
The leftmost token in a chunk has the maximum look-ahead
with access to all the future tokens in the chunk whereas the
last token has the least look-ahead with access to zero future
tokens. The average look-ahead for any token in chunk-aware
look-ahead is larger than the regular look-ahead which leads
to better accuracy with the same latency budget.
3.2. Hybrid architecture
We used a hybrid architecture which uses two decoders, one
CTC decoder and one RNNT decoder train our models. Both
decoders share a single encoder. The architecture of our hy-
brid model is shown in Figure 2. After training is done, any
of the two decoders can be used for inference. The hybrid
architecture has the following advantages over single decoder
models: 1) no need to train two separate models and saves
significant compute in our experiments as we did all of ex-
periments for both the CTC and RNNT, 2) speeds up the con-
vergence of the CTC decoder significantly which is generally
slower than RNNT decoders, and 3) improves the accuracy
of both the decoders likely due to the joint training. During
the training the losses of the CTC decoder (lctc) and RNNT
decoder (lrnnt) are mixed with a weighted summation as the
following:
ltotal = lctc + lrnnt
where ltotal is the total loss to get optimized, and is
the hyperparameter to control the balance between these two
losses.
3.3. Inference with caching
In streaming inference, we process the input in chunks. Us-
ing a larger chunk size results in higher latency but requires

===== FILE: ./2312.17279v3.pdf | PAGE: 4/8 =====

hk+2
hk+1 hk+3 hk+4 hk+5
gk
fk+3 fk+4 fk+5
gk+1 gk+2 gk+3 gk+4 gk+5
hk-1 hk
Queries
Keys
Current chunk's activations
Caches Convolution Self-attention
hk-3 hk-2 hk-1 hk hk+1 hk+2 hk+3 hk+4 hk+5 hk+6
hk-4
Chunk i Chunk i+1
Chunk i-1
gk-1
gk-2
gk-3
hk+1 hk+2
fk fk+1 fk+2
gk gk+1 gk+2
hk-3 hk-2 hk-1 hk
Fig. 3. Caching schema of self-attention and convolution lay-
ers for consecutive chunks.
fewer calls to the forward pass through the model. We use
chunk size C = M + 1, where M is the look-ahead. How-
ever, the chunks are overlapping with stride of 1 in regular
look-ahead compared to stride of M + 1 with no overlap
for chunk-aware look-ahead. The straightforward approach
to process chunks is to pass each chunk along with the effec-
tive past context. However, this approach is very inefficient
as there is a huge overlap in the computation of past con-
text. We propose a caching approach to avoid these recom-
putations and have zero duplication in streaming inference.
Normalization, feedforward, and pointwise convolution lay-
ers do not need caching as they do not require any context.
However, self-attention and depth-wise convolution with a
kernel size greater than 1 do depend on past context. There-
fore, caching intermediate activations from the processing of
previous chunks can lead to a more efficient inference.
For each causal 1D depthwise convolution with kernel
size K, we use a cache of size Cconv = K - 1. This cache
contains the activations of the last Cconv steps from the pre-
vious chunks. Initially, the cache is filled with zeros for the
first chunk. It gets updated at each streaming step as shown
in Figure 3. The cache is filled with the gk-3, gk-2, gk-1
outputs of the layer below from the previous streaming step.
In the current step, outputs gk, gk+1, gk+2 from the layer
below would be used to overwrite the previous values in
that part of the cache. The updated cache therefore contains
gk+1, gk+2, gk+3 to be used in the next streaming step. Given
a batch size of B and a model with L depth-wise convolutions
layers, each having hidden size of D, we require a cache ma-
trix of size L\times\\{\\}B \times\\{\\}D \times\\{\\}Cconv. Each layer updates the cache
matrix by storing the necessary activations in each streaming
step.
Unlike the fixed-length cache for convolution layers, the
cache size for a self-attention layer grows from zero up to the
past context size. For self-attention layers with left context of
Lc, the cache is empty in the first streaming step. With ev-
ery streaming step, chunk size number of activations from the
input to self-attention layer is added to the cache and any ex-
tra old values are dropped. Eventually, the cache grows to its
Inference Look-ahead WER, Avg Latency,
Decoder Mode Method \\% ms
CTC
Offline Full context 5.7 -
Buffered Buffering 8.0 1500
Cache-aware
Zero 10.6 0
Regular 7.9 1360
Chunk-aware 7.1 1360
RNNT
Offline Full context 5.0 -
Buffered Buffering 11.3 2000
Cache-aware
Zero 9.5 0
Regular 7.1 1360
Chunk-aware 6.3 1360
Table 1. The accuracy and average of latency for different
streaming FastConformer models trained on LibriSpeech and
evaluated on test-other set.
full size and contains the last Lc activations only. For exam-
ple, in figure 3, the cache for self-attention contains only three
values hk-3, hk-2, hk-1 since initially the cache was empty
and got updated with chunk size of three elements from pre-
vious streaming step. At the end of this step, hk, hk+1, hk+2
are added to cache which would make cache size 6. There-
fore, the two oldest values hk-3, hk-2 are dropped to main-
tain maximum cache size of Lc, here four, to be used in the
next streaming step as shown on the right. For a batch size of
B, a model with L self-attention layers, each having hidden
size of D, requires a cache matrix of size L \times\\{\\} B \times\\{\\} Cmha \times\\{\\} D
where 0 \le\\{\\} Cmha \le\\{\\} Lc.
The downsampling module uses striding convolutions
and can also benefit from caching. However, due to a small
kernel size (typically 3), it would be a small cache and can
be ignored. Instead, we can simply concatenate the last
log(Dr) 2 + 1 mel-spectrogram feature frames to each
chunk where Dr is the downsampling rate. The decoder
of FastConformer-CTC is stateless while the RNN-T de-
coder consists of RNN layers with states. Therefore, for
FastConformer-T, all the hidden states of RNN layers need
to be stored after each streaming step. In the next step, these
cached states are used to initialize all RNN layers. By main-
taining such caches, the prediction of the network would be
exactly the same as when the entire audio is processed in a
single step.
4. EXPERIMENTS
We evaluated our proposed streaming approach with hybrid
architecture of FastConformer [2]. All the results are reported
for both the the CTC and RNNT decoders which are denoted
as FastConformer-CTC and FastConformer-T respectively.
The parameter of the hybrid loss is set to 0.3 as it showed
the best performance in our experiments. We performed
all the experiments on the models which have 114M pa-

===== FILE: ./2312.17279v3.pdf | PAGE: 5/8 =====

Decoder Architecture Type Approach 0ms 40ms 240ms 520ms 680ms 1360ms
CTC Hybrid Regular 10.6 - - - 8.3 7.9
Hybrid Chunk-aware 10.6 10.1 8.8 8.4 8.0 7.1
Non-hybrid Chunk-aware 10.8 10.3 8.9 - 8.1 -
RNNT Hybrid Regular 9.5 - - - 7.6 7.1
Hybrid Chunk-aware 9.5 9.0 7.8 7.5 7.3 6.3
Non-hybrid Chunk-aware 9.4 8.9 8.3 - 7.6 -
Table 2. The accuracy (WER\\%) of cache-aware streaming FastConformer models with different latencies and look-ahead
approaches, evaluated on LibriSpeech test-other set. Not all latencies are feasible for regular look-ahead approach.
Avg. LS [21] LS [21] SPGISpeech Earnings22 GigaSpeech Tedlium MCV Voxpopuli AMI
Model Decoder Latency test-other test-clean [22] [23] [24] [25] [26] [27] [28] Averaged
Cache-aware CTC 40 7.9 3.4 7.1 22.5 16.7 7.1 15.8 9.9 29.3 13.3
Cache-aware 240 7.3 3.4 6.6 22.2 15.8 6.5 15.1 8.8 27.3 12.6
Cache-aware 520 6.2 2.6 6.1 21.2 14.3 5.8 13.6 7.8 23.3 11.2
Buffered 2000 7.7 3.6 6.8 20.1 14.0 5.5 15.9 8.8 25.3 12.0
Cache-aware RNNT 40 6.4 2.6 6.1 21.0 15.2 6.2 13.8 8.2 29.1 12.1
Cache-aware 240 5.9 2.5 5.7 20.8 14.2 5.5 13.0 7.6 26.8 11.3
Cache-aware 520 5.4 2.2 5.5 20.0 13.6 5.4 11.9 7.1 24.2 10.6
Buffered 2000 9.4 4.8 8.8 23.8 16.4 7.0 1 7.7 10.8 36.0 15.0
Table 3. The accuracy (WER\\%) of cache-aware and buffered streaming FastConformer with different look-ahead sizes and
decoders on different benchmarks. All models are trained on NeMo ASRSET 3.0.
Model Decoder 40ms 240ms 520ms
Single-Lookahead CTC 7.9 7.3 6.2
Multi-Lookahead 7.6 6.5 6.0
Single-Lookahead RNNT 6.4 5.9 5.4
Multi-Lookahead 6.2 5.5 5.2
Table 4. The comparison between single look-ahead models
vs a multi-lookahead model trained on NeMo ASRSET 3.0
for different latencies. Accuracies (WER\\%) are reported on
test-other set of LibriSpeech.
rameters. We followed the same configuration used in [2].
Experiments are done on two datasets: 1) LibriSpeech (LS)
with speed perturbation of 10\\% [21], and 2) NeMo ASRSET
3.0. NeMo ASRSET is a large multi-domain dataset which is
a collection of some publicly available speech datasets with
total size of 26K hours.
All models are trained for at least 200 epochs with effec-
tive batch size of 2048 for LibriSpeech and 4096 for NeMo
ASRSET 3.0. SentencePiece [29] with byte pair encoding
(BPE) is used as the tokenizer with vocab size of 1024, and
they are trained on the train set of each training dataset. We
trained the models with AdamW optimizer [30] with weight
decay of 0.001 and Noam scheduler [31] with coefficients of
5.0. We used checkpoint averaging of the best five check-
points based on the WER of the validation sets to get the final
models. Mixed precision training with FP16 [32] is used for
most of the experiments to speed up the training process. All
the average latencies in this paper are referring to algorithmic
latency induced by the encoder (EIL) introduced in [33]. It
is calculated as the average time needed for each word to get
predicted by the model while ignoring the inference time of
the neural network.
We used FastEmit [34] for the RNNT loss with of 0.005
to prevent the model from delaying the predictions. FastEmit
showed to be very effective and crucial to improve the accu-
racy of the streaming models for both the RNNT and CTC
decoders. This positive cross-decoder effect on the CTC de-
coder is another advantage of the hybrid architecture.
4.1. Streaming vs offline models
In this experiment, we compare different cache-aware stream-
ing models with offline models and buffered streaming. All
models are trained on LS and the results of the evaluations
on the test-other set of the LS are reported in Table 1. The
offline models are trained with unlimited context over the en-
tire audio. We evaluated and reported the performance of
these models in both full context as well as buffered stream-
ing mode. We use the buffered streaming solution as a base-
line which can be used for streaming inference with models
trained in full-context (offline) mode. In this approach, the
input is passed chunk-by-chunk but in order to get reason-
able results at the borders, we add some of the past and future
audios as context to each chunk. The total audio including
the chunk and its contexts is stored in a buffer. The contexts
would result in re-computation and waste of compute. In the
experiments for buffered streaming, we used a chunk size of
1 second and 2 seconds for the CTC and RNNT respectively
with buffer sizes of 4 seconds.
The results for the regular and chunk-aware streaming

===== FILE: ./2312.17279v3.pdf | PAGE: 6/8 =====

models are selected from the models with average latency of
1360ms. Cache-aware models show significantly better ac-
curacy with lower latency while using less computation com-
pared with the buffered approach. While some contexts are
added to each chunk in buffered streaming, not that much
duplication is needed for chunk-aware streaming models. It
makes the cache-aware streaming models significantly faster
than buffered streaming models. This speed gap can be sig-
nificantly higher with a larger buffer size or smaller chunk
size.
As it can be seen, the accuracy of the buffered streaming
for RNNT models is not as good as the CTC decoders while
they use even larger chunk size. Additionally, in our experi-
ments the performance of the buffered RNNT was not robust
to the buffer and chunk size parameters, while cache-aware
models were more robust and showed better accuracy with
lower latency.
Moreover, our streaming models show smaller accuracy
degradation from the offline model compared to buffered
streaming. The accuracy of our streaming model is exactly
the same when evaluated in offline and streaming modes
as training and evaluation have the same limited contexts.
There is inconsistency between the contexts available dur-
ing training and inference for the buffered approach. Due
to the caching mechanism, the total computation for offline
inference and streaming inference is also the same for chunk-
aware approach.
4.2. Effect of look-ahead size on accuracy
We evaluated the effect of different look-ahead sizes on the
accuracy of the proposed streaming models on LibriSpeech
dataset. The WERs on test-other set of LS for six differ-
ent lengths of look-ahead are shown in Table 2 for regular
and chunk-aware approaches. One of the disadvantages of
the regular look-ahead over the chunk-aware is that not any
look-ahead size is feasible with regular look-ahead. For Fast-
Conformer models which has 8X downsampling and window
shift of the mel spectogram input is 10ms, even one token of
look-ahead would translate into 81017 = 1360ms of look-
ahead considering all the evaluated models have 17 layers.
Results show that chunk-aware look-ahead are better than
regular look-ahead in term of accuracy with the same latency.
Additionally, it can be seen that larger look-ahead signif-
icantly improves the accuracy of both approaches, which
shows the importance of look-ahead for better accuracy with
the sacrifice of latency. The average of latency for each case
is half of the look-ahead size. In the same Table 2, we also
reported the accuracy of the same models with chunk-aware
approach trained with non-hybrid architecture to show the
effectiveness of the hybrid architecture for streaming mod-
els. As it can be seen, the hybrid variants demonstrate better
accuracy compared to non-hybrid ones.
4.3. Large scale multi-domain training
To evaluate the effectiveness of our proposed approach, we
evaluated the chunk-aware model on a large multi-domain
dataset (NeMo ASRSET 3.0). More detail on this dataset can
be found in [35].
The accuracy of both the cache-aware FastConformer-
CTC and FastConformer-T evaluated on a collection of eval-
uation sets are reported in Table 3. As expected results are
similar to the experiments on the LibriSpeech, higher latency
would result in higher accuracy, and RNNT-based models are
better than their equivalent CTC. As it can be seen, the cache-
aware streaming models outperform the buffered streaming
models on all benchmarks.
4.4. Multiple look-ahead training
One of the disadvantages of the cache-aware streaming com-
paring to buffered streaming is that each model is trained for a
specific latency and supporting multiple latencies need train-
ing of multiple models. In order to address this shortcom-
ing, we proposed to train the streaming model with multiple
latencies. For each batch on each GPU, we randomly se-
lect a chunk size and it makes the model to support differ-
ent latencies. To evaluate the proposed approach, we trained
a chunk-aware model with multiple latencies and compared
the averaged accuracy on all benchmarks to models trained
with single latency. The benchmarks are the same as the ones
used in Table 3. The multi-lookahead model may need more
steps to achieve the same accuracy as training a single latency
model. The results are reported in Table 4 for both the CTC
and RNNT decoders. The multi-lookahead model even shows
better accuracy than single lookahead models while just one
model is trained for multiple latencies. The training on multi-
ple look-aheads have helped the model to become more robust
and even achieve better accuracy in some cases.
5. CONCLUSION
We proposed a streaming ASR model based on FastCon-
former where the non-autoregressive encoder is converted
into an autoregressive recurrent model during inference. It
is done by using an activation cache to keep the intermedi-
ate activations that are reused in future steps. The caching
drastically reduces computation cost when compared to tradi-
tional buffer-based methods while the model is still trained in
non-autoregressive mode. We evaluated our proposed model
on LibriSpeech and a large multi-domain dataset and showed
that the proposed model outperforms buffered streaming in
terms of accuracy, inference time, and latency. We also intro-
duced a hybrid CTC/RNNT architecture to train the streaming
models which not only saved compute but also improved the
accuracy. Additionally our experiments showed that a model
trained with multiple latencies can achieve even better accu-
racy than models trained with single latency.

===== FILE: ./2312.17279v3.pdf | PAGE: 7/8 =====

6. REFERENCES
[1] Yanzhang He, Tara N Sainath, Rohit Prabhavalkar, Ian
McGraw, Raziel Alvarez, Ding Zhao, David Rybach,
Anjuli Kannan, Yonghui Wu, Ruoming Pang, et al.,
``Streaming end-to-end speech recognition for mobile
devices,'' in ICASSP, 2019.
[2] Dima Rekesh, Nithin Rao Koluguri, Samuel Kriman,
Somshubra Majumdar, Vahid Noroozi, He Huang, Olek-
sii Hrinchuk, Krishna Puvvada, Ankur Kumar, Ja-
gadeesh Balam, and Boris Ginsburg, ``Fast conformer
with linearly scalable attention for efficient speech
recognition,'' ASRU, 2023.
[3] Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki
Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang,
Zhengdong Zhang, Yonghui Wu, et al., ``Conformer:
Convolution-augmented transformer for speech recog-
nition,'' InterSpeech, 2020.
[4] Alex Graves, Santiago Fernandez, Faustino Gomez, and
Jurgen Schmidhuber, ``Connectionist temporal classifi-
cation: labelling unsegmented sequence data with recur-
rent neural networks,'' in ICML, 2006.
[5] Alex Graves, ``Sequence transduction with recurrent
neural networks,'' arXiv e-prints, pp. arXiv--1211, 2012.
[6] Oleksii Kuchaiev, Jason Li, Huyen Nguyen, Oleksii
Hrinchuk, Ryan Leary, Boris Ginsburg, Samuel Kriman,
Stanislav Beliaev, Vitaly Lavrukhin, Jack Cook, et al.,
``Nemo: a toolkit for building ai applications using neu-
ral modules,'' arXiv preprint arXiv:1909.09577, 2019.
[7] Qian Zhang, Han Lu, Hasim Sak, Anshuman Tripathi,
Erik McDermott, Stephen Koo, and Shankar Kumar,
``Transformer transducer: A streamable speech recog-
nition model with transformer encoders and rnn-t loss,''
in ICASSP, 2020.
[8] Niko Moritz, Takaaki Hori, and Jonathan Le, ``Stream-
ing automatic speech recognition with the transformer
model,'' in ICASSP, 2020.
[9] Xie Chen, Yu Wu, Zhenghao Wang, Shujie Liu, and
Jinyu Li, ``Developing real-time streaming trans-
former transducer for speech recognition on large-scale
dataset,'' in ICASSP, 2021.
[10] Emiru Tsunoo, Yosuke Kashiwagi, and Shinji Watan-
abe, ``Streaming transformer asr with blockwise syn-
chronous beam search,'' in Spoken Language Technol-
ogy Workshop (SLT), 2021.
[11] Chunyang Wu, Yongqiang Wang, Yangyang Shi, Ching-
Feng Yeh, and Frank Zhang, ``Streaming transformer-
based acoustic models using self-attention with aug-
mented memory,'' InterSpeech, 2020.
[12] Hirofumi Inaguma, Masato Mimura, and Tatsuya Kawa-
hara, ``Enhancing monotonic multihead attention for
streaming asr,'' InterSpeech, 2020.
[13] Bo Li, Anmol Gulati, Jiahui Yu, Tara N Sainath, Chung-
Cheng Chiu, Arun Narayanan, Shuo-Yiin Chang,
Ruoming Pang, Yanzhang He, James Qin, et al., ``A
better and faster end-to-end model for streaming asr,'' in
ICASSP, 2021.
[14] Jiahui Yu, Wei Han, Anmol Gulati, Chung-Cheng Chiu,
Bo Li, Tara N Sainath, Yonghui Wu, and Ruoming
Pang, ``Dual-mode ASR: Unify and improve streaming
asr with full-context modeling,'' ICLR, 2021.
[15] Zhuoyuan Yao, Di Wu, Xiong Wang, Binbin Zhang,
Fan Yu, Chao Yang, Zhendong Peng, Xiaoyu Chen, Lei
Xie, and Xin Lei, ``Wenet: Production oriented stream-
ing and non-streaming end-to-end speech recognition
toolkit,'' arXiv:2102.01547, 2021.
[16] Sergey Ioffe and Christian Szegedy, ``Batch normaliza-
tion: Accelerating deep network training by reducing
internal covariate shift,'' in ICML. PMLR, 2015.
[17] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-
ton, ``Layer normalization,'' NIPS, 2016.
[18] Kwangyoun Kim, Felix Wu, Prashant Sridhar, Kyu J.
Han, and Shinji Watanabe, ``Multi-mode transformer
transducer with stochastic future context,'' in Inter-
speech, 2021.
[19] Chengyi Wang, Yu Wu, Shujie Liu, Jinyu Li, Liang Lu,
Guoli Ye, and Ming Zhou, ``Low latency end-to-end
streaming speech recognition with a scout network,'' In-
terSpeech, 2020.
[20] Zhengkun Tian, Jiangyan Yi, Ye Bai, Jianhua Tao, Shuai
Zhang, and Zhengqi Wen, ``Synchronous transformers
for end-to-end speech recognition,'' in ICASSP, 2020.
[21] Vassil Panayotov, Guoguo Chen, Daniel Povey, and San-
jeev Khudanpur, ``Librispeech: an asr corpus based on
public domain audio books,'' in ICASSP, 2015.
[22] Patrick K O'Neill, Vitaly Lavrukhin, Somshubra
Majumdar, Vahid Noroozi, Yuekai Zhang, Olek-
sii Kuchaiev, Jagadeesh Balam, Yuliya Dovzhenko,
Keenan Freyberg, Michael D Shulman, et al., ``Spgis-
peech: 5,000 hours of transcribed financial audio for
fully formatted end-to-end speech recognition,'' in In-
terSpeech, 2021.
[23] Miguel Del Rio, Peter Ha, Quinten McNamara, Corey
Miller, and Shipra Chandra, ``Earnings-22: A practi-
cal benchmark for accents in the wild,'' arXiv preprint
arXiv:2203.15591, 2022.

===== FILE: ./2312.17279v3.pdf | PAGE: 8/8 =====

[24] Guoguo Chen, Shuzhou Chai, Guanbo Wang, Jiayu Du,
Wei Qiang Zhang, Chao Weng, Dan Su, Daniel Povey,
Jan Trmal, Junbo Zhang, et al., ``Gigaspeech: An evolv-
ing, multi-domain asr corpus with 10,000 hours of tran-
scribed audio,'' in InterSpeech, 2021.
[25] Anthony Rousseau, Paul Deleglise, and Yannick Esteve,
``Ted-lium: An automatic speech recognition dedicated
corpus,'' in LREC, 2012.
[26] Rosana Ardila, Megan Branson, Kelly Davis, Michael
Henretty, Michael Kohler, Josh Meyer, Reuben Morais,
Lindsay Saunders, Francis M Tyers, and Gregor Weber,
``Common voice: A massively-multilingual speech cor-
pus,'' LREC, 2020.
[27] Changhan Wang, Morgane Riviere, Ann Lee, Anne Wu,
Chaitanya Talnikar, Daniel Haziza, Mary Williamson,
Juan Pino, and Emmanuel Dupoux, ``Voxpopuli: A
large-scale multilingual speech corpus for representa-
tion learning, semi-supervised learning and interpreta-
tion,'' in ACL, 2021.
[28] Wessel Kraaij, Thomas Hain, Mike Lincoln, and Wil-
fried Post, ``The ami meeting corpus,'' in International
Conference on Methods and Techniques in Behavioral
Research, 2005.
[29] Taku Kudo and John Richardson, ``Sentencepiece: A
simple and language independent subword tokenizer
and detokenizer for neural text processing,'' EMNLP,
2018.
[30] Ilya Loshchilov and Frank Hutter, ``Decoupled weight
decay regularization,'' arXiv:1711.05101, 2017.
[31] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, ukasz Kaiser,
and Illia Polosukhin, ``Attention is all you need,''
NeurIPS, 2017.
[32] Paulius Micikevicius, Sharan Narang, Jonah Alben,
Gregory Diamos, Erich Elsen, David Garcia, Boris
Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh
Venkatesh, et al., ``Mixed precision training,'' in ICLR,
2018.
[33] Yangyang Shi, Yongqiang Wang, Chunyang Wu, Ching-
Feng Yeh, Julian Chan, Frank Zhang, Duc Le, and Mike
Seltzer, ``Emformer: Efficient memory transformer
based acoustic model for low latency streaming speech
recognition,'' in ICASSP, 2021.
[34] Jiahui Yu, Chung-Cheng Chiu, Bo Li, Shuo-yiin Chang,
Tara N Sainath, Yanzhang He, Arun Narayanan, Wei
Han, Anmol Gulati, Yonghui Wu, et al., ``Fastemit:
Low-latency streaming asr with sequence-level emis-
sion regularization,'' in ICASSP, 2021.
[35] NVIDIA-NeMo, ``FastConformer Hybrid Large Stream-
ing Multi (en-US),'' https://huggingface.co/
nvidia/stt\\_en\\_fastconformer\\_hybrid\\_
large\\_streaming\\_multi, 2023.
